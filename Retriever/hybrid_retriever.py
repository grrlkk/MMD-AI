# import os
# import sys
# import torch

# # ë¦¬ë­í‚¹
# from sentence_transformers import CrossEncoder

# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# from DB.opensearch import OpenSearchDB
# from langchain_huggingface import HuggingFaceEmbeddings

# # --- ì „ì—­ ëª¨ë¸ ë³€ìˆ˜ ---
# # ê¸°ì¡´ ì „ì—­ ë³€ìˆ˜
# _embedding_model = None

# # ë¦¬ë­í‚¹ ëª¨ë¸ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
# _reranker_model = None

# # --- ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ ---
# def get_embedding_model():
#     """ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
#     global _embedding_model
#     if _embedding_model is None:
#         print("Initializing embedding model for hybrid search...")
#         _embedding_model = HuggingFaceEmbeddings(
#             model_name="intfloat/multilingual-e5-large",
#             model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
#             encode_kwargs={'normalize_embeddings': True}
#         )
#         print(f"âœ… Embedding model initialized on {'cuda' if torch.cuda.is_available() else 'cpu'}")
#     return _embedding_model

# # â–¼â–¼â–¼â–¼â–¼ ë¦¬ë­í‚¹ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ â–¼â–¼â–¼â–¼â–¼
# def get_reranker_model():
#     """CrossEncoder ë¦¬ë­ì»¤ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
#     global _reranker_model
#     if _reranker_model is None:
#         print("Initializing reranker model...")
#         _reranker_model = CrossEncoder(
#             'BAAI/bge-reranker-base',
#             max_length=512,
#             device='cuda' if torch.cuda.is_available() else 'cpu'
#         )
#         print(f"âœ… Reranker model initialized on {'cuda' if torch.cuda.is_available() else 'cpu'}")
#     return _reranker_model


# # --- ì¿¼ë¦¬ ìƒì„± ë° í¬ë§·íŒ… í•¨ìˆ˜ ---
# def build_hybrid_query(user_input: dict, top_k: int = 5, exclude_ids: list = None) -> dict:
#     if exclude_ids is None:
#         exclude_ids = []

#     major = user_input.get("candidate_major", "")
#     interest = user_input.get("candidate_interest", "")
#     career = user_input.get("candidate_career", "")
#     tech_stack = " ".join(user_input.get("candidate_tech_stack", []))
#     location = user_input.get("candidate_location", "")
#     query = user_input.get("candidate_question", "")

#     embedding_model = get_embedding_model()
#     query_vector = embedding_model.embed_query(f"query: {query}")

#     must_not_clauses = []
#     if exclude_ids:
#         must_not_clauses.append({"ids": {"values": exclude_ids}})

#     search_query = {
#         "query": { "bool": {
#                 "should": [
#                     { "multi_match": { "query": interest, "fields": ["job_name^3", "title^2", "position_detail"], "boost": 3.0 }},
#                     { "multi_match": { "query": tech_stack, "fields": ["position_detail", "preferred_qualifications", "qualifications"], "boost": 2.5 }},
#                     { "multi_match": { "query": f"{major}", "fields": ["qualifications", "preferred_qualifications"], "boost": 1.5 }},
#                     { "match": {"career": {"query": career, "boost": 1.5}}},
#                     { "match": {"location": {"query": location, "boost": 1.2}}} if location else None,
#                     { "knn": { "content_embedding": { "vector": query_vector, "k": top_k * 2, "boost": 2.0 }}}
#                 ],
#                 "must_not": must_not_clauses,
#                 "minimum_should_match": 1
#             }},
#         "size": top_k,
#         "_source": { "excludes": ["content_embedding"] }
#     }
#     search_query["query"]["bool"]["should"] = [q for q in search_query["query"]["bool"]["should"] if q is not None]
#     return search_query


# def _format_hit_to_text(hit_source: dict) -> str:
#     if not hit_source:
#         return ""
#     field_order_map = [
#         ('title', 'ì§ë¬´'), ('company_name', 'íšŒì‚¬'), ('job_category', 'ì§ë¬´ ì¹´í…Œê³ ë¦¬'),
#         ('location', 'ìœ„ì¹˜'), ('career', 'ê²½ë ¥'), ('dead_line', 'ë§ˆê°ì¼'),
#         ('position_detail', 'í¬ì§€ì…˜ ìƒì„¸'), ('main_tasks', 'ì£¼ìš” ì—…ë¬´'),
#         ('qualifications', 'ìê²© ìš”ê±´'), ('preferred_qualifications', 'ìš°ëŒ€ ì‚¬í•­'),
#         ('benefits', 'í˜œíƒ ë° ë³µì§€'), ('hiring_process', 'ì±„ìš© ê³¼ì •'), ('url', 'ì±„ìš©ê³µê³  URL')
#     ]
#     lines = ["[document]"]
#     for field_key, display_name in field_order_map:
#         value = hit_source.get(field_key)
#         if value:
#             if field_key in ['main_tasks', 'qualifications', 'preferred_qualifications', 'benefits'] and isinstance(value, list):
#                 formatted_value = '\n'.join([f"- {item}" for item in value])
#                 lines.append(f"{display_name}:\n{formatted_value}")
#             elif isinstance(value, list):
#                 lines.append(f"{display_name}: {', '.join(value)}")
#             else:
#                 lines.append(f"{display_name}: {value}")
#     return "\n\n".join(lines)


# # --- ë©”ì¸ ê²€ìƒ‰ í•¨ìˆ˜ (use_reranker í”Œë˜ê·¸ ì¶”ê°€) ---
# def hybrid_search(user_profile: dict, top_k: int = 5, exclude_ids: list = None, use_reranker: bool = True) -> tuple[list[float], list[str], list[dict]]:
#     """
#     í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , use_reranker í”Œë˜ê·¸ì— ë”°ë¼ ì„ íƒì ìœ¼ë¡œ ë¦¬ë­í‚¹ì„ ì ìš©í•©ë‹ˆë‹¤.
#     """
#     opensearch = OpenSearchDB()

#     # ë¦¬ë­ì»¤ë¥¼ ì‚¬ìš©í•  ê²½ìš° ë” ë§ì€ í›„ë³´êµ°(retrieval_k)ì„ ê°€ì ¸ì˜¤ê³ , ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ top_kë§Œ ê°€ì ¸ì˜´
#     retrieval_k = top_k * 5 if use_reranker else top_k
    
#     print(f"ğŸ” 1ë‹¨ê³„ (Retrieval): OpenSearchì—ì„œ í›„ë³´êµ° {retrieval_k}ê°œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. (ë¦¬ë­í‚¹: {'ON' if use_reranker else 'OFF'})")
#     try:
#         search_query = build_hybrid_query(user_profile, retrieval_k, exclude_ids)
#         response = opensearch.search(search_query, size=retrieval_k)
#     except Exception as e:
#         print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
#         return [], [], []

#     initial_hits = response.get("hits", {}).get("hits", [])
#     if not initial_hits:
#         return [], [], []
#     print(f"âœ… 1ë‹¨ê³„ (Retrieval) ì™„ë£Œ: {len(initial_hits)}ê°œì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")

#     # --- 2ë‹¨ê³„: ë¦¬ë­í‚¹ (use_rerankerê°€ Trueì¼ ë•Œë§Œ ì‹¤í–‰) ---
#     if use_reranker:
#         print("\nğŸ”„ 2ë‹¨ê³„ (Reranking): ê°€ì ¸ì˜¨ ê²°ê³¼ì˜ ìˆœìœ„ë¥¼ ì¬ì¡°ì •í•©ë‹ˆë‹¤.")
#         reranker = get_reranker_model()
#         rerank_query = f"{user_profile.get('candidate_interest', '')} {user_profile.get('candidate_question', '')}"
#         sentence_pairs = [[rerank_query, _format_hit_to_text(hit.get('_source', {}))] for hit in initial_hits]
#         rerank_scores = reranker.predict(sentence_pairs, show_progress_bar=False)
#         reranked_results = sorted(zip(rerank_scores, initial_hits), key=lambda x: x[0], reverse=True)
        
#         final_results = reranked_results[:top_k]
        
#         scores = [float(score) for score, hit in final_results] # score íƒ€ì…ì„ floatë¡œ í†µì¼
#         documents = [hit.get("_source", {}) for score, hit in final_results]
#         doc_ids = [hit.get("_id", "") for score, hit in final_results]
        
#         print(f"âœ… 2ë‹¨ê³„ (Reranking) ì™„ë£Œ: ìµœì¢… {len(scores)}ê°œì˜ ê²°ê³¼ê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
#         return scores, doc_ids, documents
    
#     # --- ë¦¬ë­í‚¹ì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²½ìš° ---
#     else:
#         # OpenSearchì˜ ì ìˆ˜ì™€ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
#         scores = [hit.get("_score", 0.0) for hit in initial_hits]
#         documents = [hit.get("_source", {}) for hit in initial_hits]
#         doc_ids = [hit.get("_id", "") for hit in initial_hits]
#         return scores, doc_ids, documents


# # --- ì‹¤í–‰ ë¶€ë¶„ ---
# if __name__ == "__main__":
#     base_user_info = {
#         "user_id": 10,
#         "candidate_major": "ê²½ì˜í•™",
#         "candidate_interest": "ì„œë¹„ìŠ¤ ê¸°íšì",
#         "candidate_career": "5ë…„",
#         "candidate_tech_stack": [
#             "UX/UI ì„¤ê³„", "ë°ì´í„° ë¶„ì„", "A/B í…ŒìŠ¤íŠ¸", "í”„ë¡œì íŠ¸ ê´€ë¦¬"
#         ],
#         "candidate_location": "ì„œìš¸ ê°•ë‚¨",
#         "candidate_question": "ì•ˆë…•í•˜ì„¸ìš”, ë°ì´í„° ê¸°ë°˜ì˜ ì˜ì‚¬ê²°ì •ì„ ì¤‘ìš”í•˜ê²Œ ì—¬ê¸°ëŠ” ì„±ì¥í•˜ëŠ” ìŠ¤íƒ€íŠ¸ì—…ì—ì„œ ì„œë¹„ìŠ¤ ê¸°íšì ì§ë¬´ì— ì í•©í•œ í¬ì§€ì…˜ì´ ìˆì„ê¹Œìš”?"
#     }

#     print("\n=== í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ë¦¬ë­í‚¹ ê²°ê³¼ ===")
#     scores, doc_ids, documents = hybrid_search(base_user_info, top_k=5)

#     if not scores:
#         print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
#     else:
#         for i, (score, doc_id, document) in enumerate(zip(scores, doc_ids, documents), 1):
#             print(f"\n[ìµœì¢… ìˆœìœ„ {i}] Rerank Score: {score:.4f}, ë¬¸ì„œ ID: {doc_id}")
#             # print(document) # ì „ì²´ ë¬¸ì„œë¥¼ ë³´ë ¤ë©´ ì£¼ì„ í•´ì œ
#             print(f"  ì œëª©: {document.get('title', 'ì •ë³´ ì—†ìŒ')}")
#             print(f"  íšŒì‚¬ëª…: {document.get('company_name', 'ì •ë³´ ì—†ìŒ')}")
#             print(f"  ì§€ì—­: {document.get('location', 'ì •ë³´ ì—†ìŒ')}")
#             print(f"  ì£¼ìš” ì—…ë¬´: {document.get('main_tasks', 'ì •ë³´ ì—†ìŒ')}")
#             print("-" * 50)


import os
import sys
import torch
import re
from typing import Union # 1. Union ì„í¬íŠ¸ ì¶”ê°€

# ë¦¬ë­í‚¹
from sentence_transformers import CrossEncoder

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from DB.opensearch import OpenSearchDB
from langchain_huggingface import HuggingFaceEmbeddings

# --- ì „ì—­ ëª¨ë¸ ë³€ìˆ˜ ---
_embedding_model = None
_reranker_model = None

# --- ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ ---
def get_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _embedding_model
    if _embedding_model is None:
        print("Initializing embedding model for hybrid search...")
        _embedding_model = HuggingFaceEmbeddings(
            model_name="intfloat/multilingual-e5-large",
            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print(f"âœ… Embedding model initialized on {'cuda' if torch.cuda.is_available() else 'cpu'}")
    return _embedding_model

def get_reranker_model():
    """CrossEncoder ë¦¬ë­ì»¤ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _reranker_model
    if _reranker_model is None:
        print("Initializing reranker model...")
        _reranker_model = CrossEncoder(
            'BAAI/bge-reranker-base',
            max_length=512,
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )
        print(f"âœ… Reranker model initialized on {'cuda' if torch.cuda.is_available() else 'cpu'}")
    return _reranker_model

# --- ì¿¼ë¦¬ ìƒì„± ë° í¬ë§·íŒ… í•¨ìˆ˜ ---

def _get_years_from_career(career: str) -> int:
    """'në…„', 'n ë…„' í˜•ì‹ì—ì„œ ìˆ«ì nì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # ìˆ«ìë¥¼ ì°¾ê¸° ìœ„í•œ ì •ê·œì‹
    match = re.search(r'(\d+)', career)
    if match:
        return int(match.group(1))
    return 0 # ìˆ«ìê°€ ì—†ìœ¼ë©´ 0ë…„(ì‹ ì…)ìœ¼ë¡œ ê°„ì£¼

def _build_career_filter(career: str) -> Union[dict, None]:
    """[ì—…ê·¸ë ˆì´ë“œ ë²„ì „] ì‚¬ìš©ìì˜ career ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§€ëŠ¥ì ì¸ OpenSearch filter ì ˆì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if not career:
        return None

    user_years = _get_years_from_career(career)

    # CASE 1: ì‚¬ìš©ìê°€ 'ì‹ ì…'ì´ê±°ë‚˜ ê²½ë ¥ì— ìˆ«ìê°€ ì—†ëŠ” ê²½ìš°
    if user_years == 0 and ("ì‹ ì…" in career or "ë¬´ê´€" in career):
        print("â„¹ï¸ 'ì‹ ì…/ê²½ë ¥ë¬´ê´€' í•„í„°ë¥¼ ì ìš©í•©ë‹ˆë‹¤.")
        # 'ì‹ ì…', 'ê²½ë ¥ë¬´ê´€', 'ë¬´ê´€' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œë¥¼ ëª¨ë‘ ì°¾ë„ë¡ ìœ ì—°í•˜ê²Œ ì„¤ì •
        return {
            "bool": {
                "should": [
                    {"match": {"career": "ì‹ ì…"}},
                    {"match": {"career": "ë¬´ê´€"}},
                    {"match": {"career": "ê²½ë ¥ë¬´ê´€"}}
                ],
                "minimum_should_match": 1
            }
        }

    # CASE 2: ì‚¬ìš©ìê°€ ê²½ë ¥ì§ì¸ ê²½ìš° (ìˆ«ì ë…„ì°¨ ì…ë ¥)
    if user_years > 0:
        print(f"â„¹ï¸ '{user_years}ë…„ì°¨' ê²½ë ¥ í•„í„°ë¥¼ ì ìš©í•©ë‹ˆë‹¤. ('ì‹ ì…' ê³µê³  ì œì™¸)")
        return {
            "bool": {
                # 'must' ì¡°ê±´: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê²½ë ¥ í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ì•¼ í•¨
                "must": [
                    {"match": {"career": career}}
                ],
                # 'must_not' ì¡°ê±´: 'ì‹ ì…'ì´ë¼ëŠ” ë‹¨ì–´ê°€ í¬í•¨ëœ ê³µê³ ëŠ” ì œì™¸
                "must_not": [
                    {"match": {"career": "ì‹ ì…"}}
                ]
            }
        }
    
    # ìœ„ ì¡°ê±´ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ í•„í„°ë¥¼ ì ìš©í•˜ì§€ ì•ŠìŒ
    return None

def build_hybrid_query(user_input: dict, top_k: int = 5, exclude_ids: list = None) -> dict:
    if exclude_ids is None:
        exclude_ids = []

    major = user_input.get("candidate_major", "")
    interest = user_input.get("candidate_interest", "")
    career = user_input.get("candidate_career", "")
    tech_stack = " ".join(user_input.get("candidate_tech_stack", []))
    location = user_input.get("candidate_location", "")
    query = user_input.get("candidate_question", "")

    embedding_model = get_embedding_model()
    query_vector = embedding_model.embed_query(f"query: {query}")

    must_not_clauses = []
    if exclude_ids:
        must_not_clauses.append({"ids": {"values": exclude_ids}})

    filter_clauses = []
    career_filter = _build_career_filter(career)
    if career_filter:
        filter_clauses.append(career_filter)

    search_query = {
        "query": { "bool": {
            "should": [
                { "multi_match": { "query": interest, "fields": ["job_name^3", "title^2", "position_detail"], "boost": 3.0 }},
                { "multi_match": { "query": tech_stack, "fields": ["position_detail", "preferred_qualifications", "qualifications"], "boost": 2.5 }},
                { "multi_match": { "query": f"{major}", "fields": ["qualifications", "preferred_qualifications"], "boost": 1.5 }},
                { "match": {"location": {"query": location, "boost": 1.2}}} if location else None,
                { "knn": { "content_embedding": { "vector": query_vector, "k": top_k * 2, "boost": 2.0 }}}
            ],
            "filter": filter_clauses,
            "must_not": must_not_clauses,
            "minimum_should_match": 1
        }},
        "size": top_k,
        "_source": { "excludes": ["content_embedding"] }
    }
    search_query["query"]["bool"]["should"] = [q for q in search_query["query"]["bool"]["should"] if q is not None]
    return search_query


def _format_hit_to_text(hit_source: dict) -> str:
    if not hit_source:
        return ""
    field_order_map = [
        ('title', 'ì§ë¬´'), ('company_name', 'íšŒì‚¬'), ('job_category', 'ì§ë¬´ ì¹´í…Œê³ ë¦¬'),
        ('location', 'ìœ„ì¹˜'), ('career', 'ê²½ë ¥'), ('dead_line', 'ë§ˆê°ì¼'),
        ('position_detail', 'í¬ì§€ì…˜ ìƒì„¸'), ('main_tasks', 'ì£¼ìš” ì—…ë¬´'),
        ('qualifications', 'ìê²© ìš”ê±´'), ('preferred_qualifications', 'ìš°ëŒ€ ì‚¬í•­'),
        ('benefits', 'í˜œíƒ ë° ë³µì§€'), ('hiring_process', 'ì±„ìš© ê³¼ì •'), ('url', 'ì±„ìš©ê³µê³  URL')
    ]
    lines = ["[document]"]
    for field_key, display_name in field_order_map:
        value = hit_source.get(field_key)
        if value:
            if field_key in ['main_tasks', 'qualifications', 'preferred_qualifications', 'benefits'] and isinstance(value, list):
                formatted_value = '\n'.join([f"- {item}" for item in value])
                lines.append(f"{display_name}:\n{formatted_value}")
            elif isinstance(value, list):
                lines.append(f"{display_name}: {', '.join(value)}")
            else:
                lines.append(f"{display_name}: {value}")
    return "\n\n".join(lines)


def hybrid_search(user_profile: dict, top_k: int = 5, exclude_ids: list = None, use_reranker: bool = True) -> tuple[list[float], list[str], list[dict]]:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , use_reranker í”Œë˜ê·¸ì— ë”°ë¼ ì„ íƒì ìœ¼ë¡œ ë¦¬ë­í‚¹ì„ ì ìš©í•©ë‹ˆë‹¤.
    """
    opensearch = OpenSearchDB()

    retrieval_k = top_k * 5 if use_reranker else top_k
    
    print(f"ğŸ” 1ë‹¨ê³„ (Retrieval): OpenSearchì—ì„œ í›„ë³´êµ° {retrieval_k}ê°œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. (ë¦¬ë­í‚¹: {'ON' if use_reranker else 'OFF'})")
    try:
        search_query = build_hybrid_query(user_profile, retrieval_k, exclude_ids)
        response = opensearch.search(search_query, size=retrieval_k)
    except Exception as e:
        print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return [], [], []

    initial_hits = response.get("hits", {}).get("hits", [])
    if not initial_hits:
        return [], [], []
    print(f"âœ… 1ë‹¨ê³„ (Retrieval) ì™„ë£Œ: {len(initial_hits)}ê°œì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")

    if use_reranker:
        print("\nğŸ”„ 2ë‹¨ê³„ (Reranking): ê°€ì ¸ì˜¨ ê²°ê³¼ì˜ ìˆœìœ„ë¥¼ ì¬ì¡°ì •í•©ë‹ˆë‹¤.")
        reranker = get_reranker_model()
        rerank_query = f"{user_profile.get('candidate_interest', '')} {user_profile.get('candidate_question', '')}"
        sentence_pairs = [[rerank_query, _format_hit_to_text(hit.get('_source', {}))] for hit in initial_hits]
        rerank_scores = reranker.predict(sentence_pairs, show_progress_bar=False)
        reranked_results = sorted(zip(rerank_scores, initial_hits), key=lambda x: x[0], reverse=True)
        
        final_results = reranked_results[:top_k]
        
        scores = [float(score) for score, hit in final_results]
        documents = [hit.get("_source", {}) for score, hit in final_results]
        doc_ids = [hit.get("_id", "") for score, hit in final_results]
        
        print(f"âœ… 2ë‹¨ê³„ (Reranking) ì™„ë£Œ: ìµœì¢… {len(scores)}ê°œì˜ ê²°ê³¼ê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
        return scores, doc_ids, documents
    
    else:
        scores = [hit.get("_score", 0.0) for hit in initial_hits]
        documents = [hit.get("_source", {}) for hit in initial_hits]
        doc_ids = [hit.get("_id", "") for hit in initial_hits]
        return scores, doc_ids, documents


# --- ì‹¤í–‰ ë¶€ë¶„ ---
if __name__ == "__main__":
    base_user_info = {
        "user_id": 10,
        "candidate_major": "ê°œë°œ",
        "candidate_interest": "ë°±ì—”ë“œ ê°œë°œ",
        "candidate_career": "ì‹ ì…",
        "candidate_tech_stack": [
            "python", "sql", "java", "docker"
        ],
        "candidate_location": "ì„œìš¸, ê²½ê¸°",
        "candidate_question": "ì•ˆë…•í•˜ì„¸ìš”, ì»´í“¨í„° ê³µí•™ì„ ì „ê³µí•˜ê³  ë°±ì—”ë“œ ê°œë°œìë¡œ ë‹¤ìˆ˜ì˜ í”„ë¡œì íŠ¸ì— ì°¸ì—¬í–ˆìŠµë‹ˆë‹¤. ê²½ë ¥ì€ ì—†ì§€ë§Œ ì—´ì‹¬íˆ ì¼í•  ìˆ˜ ìˆëŠ” ê³µê³ ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤."
    }

    print("\n=== í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ë¦¬ë­í‚¹ ê²°ê³¼ ===")
    scores, doc_ids, documents = hybrid_search(base_user_info, top_k=5)

    if not scores:
        print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for i, (score, doc_id, document) in enumerate(zip(scores, doc_ids, documents), 1):
            print(f"\n[ìµœì¢… ìˆœìœ„ {i}] Rerank Score: {score:.4f}, ë¬¸ì„œ ID: {doc_id}")
            print(f"  ì œëª©: {document.get('title', 'ì •ë³´ ì—†ìŒ')}")
            print(f"  íšŒì‚¬ëª…: {document.get('company_name', 'ì •ë³´ ì—†ìŒ')}")
            print(f"  ì§€ì—­: {document.get('location', 'ì •ë³´ ì—†ìŒ')}")
            print(f"  ê²½ë ¥: {document.get('career', 'ì •ë³´ ì—†ìŒ')}")
            print(f"  ì£¼ìš” ì—…ë¬´: {document.get('main_tasks', 'ì •ë³´ ì—†ìŒ')}")
            print("-" * 50)